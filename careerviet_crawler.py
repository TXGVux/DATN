import requests
import pandas as pd
import time
import random

from careerviet_crawler_job import create_browser, extract_job_details

# H√†m g·ªçi API l·∫•y danh s√°ch ƒë∆∞·ªùng d·∫´n c√¥ng vi·ªác (c√≥ debug l·ªói)
def fetch_job_links(page=1, page_size=100):
    api_url = f"https://api.careerbuilder.vn/v2/job-search?category=0&keyword=&page={page}&pageSize={page_size}"
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/json",
        "Referer": "https://careerviet.vn/",
        "Origin": "https://careerviet.vn"
    }
    res = requests.get(api_url, headers=headers)

    if res.status_code != 200:
        print(f"‚ùå L·ªói HTTP {res.status_code}: {res.text}")
        return []

    try:
        data = res.json()
    except Exception as e:
        print(f"‚ùå JSON Decode l·ªói: {e}")
        print("üìÑ N·ªôi dung ph·∫£n h·ªìi:", res.text[:500])
        return []

    jobs = data.get("data", {}).get("jobs", [])
    job_links = [
        f"https://careerviet.vn/vi/tim-viec-lam/{job['seoAlias']}.{job['jobId']}.html"
        for job in jobs
    ]
    return job_links

# Crawl to√†n b·ªô t·ª´ API danh s√°ch
if __name__ == "__main__":
    job_urls = fetch_job_links()
    print(f"‚úÖ ƒê√£ t√¨m th·∫•y {len(job_urls)} ƒë∆∞·ªùng d·∫´n vi·ªác l√†m")

    browser = create_browser()
    all_jobs = []
    for idx, job_url in enumerate(job_urls):
        print(f"‚û°Ô∏è ƒêang x·ª≠ l√Ω job {idx+1}/{len(job_urls)}: {job_url}")
        try:
            job_data = extract_job_details(browser, job_url)
            all_jobs.append(job_data)
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω {job_url}: {e}")
        time.sleep(random.uniform(2, 4))

    browser.quit()

    # Ghi v√†o file CSV
    df = pd.DataFrame(all_jobs)
    df.to_csv("all_job_details.csv", index=False, encoding="utf-8-sig")
    print("‚úÖ ƒê√£ l∆∞u to√†n b·ªô k·∫øt qu·∫£ v√†o all_job_details.csv")
