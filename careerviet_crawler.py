from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium_stealth import stealth
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from urllib.parse import urljoin, urlparse, parse_qs, urlencode, urlunparse
from twocaptcha import TwoCaptcha
import pandas as pd
import time
import random
import os


solver = TwoCaptcha('530a28a37a5a788e2a06315b2d5d17e2')  # API key 2Captcha


def create_browser(proxy=None, proxy_type='http'):
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument('--ignore-certificate-errors')
    chrome_options.add_argument('--ignore-ssl-errors')

    if proxy:
        proxy_scheme = proxy_type.lower()
        chrome_options.add_argument(f'--proxy-server={proxy_scheme}://{proxy}')

    browser = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)

    stealth(browser,
            languages=["vi-VN", "vi"],
            vendor="Google Inc.",
            platform="Win32",
            webgl_vendor="Intel Inc.",
            renderer="Intel Iris OpenGL Engine",
            fix_hairline=True,
            )
    return browser


def solve_recaptcha(site_key, url):
    print("ğŸ§© Äang giáº£i reCAPTCHA báº±ng 2Captcha...")
    try:
        result = solver.recaptcha(sitekey=site_key, url=url)
        code = result['code']
        print("âœ… Giáº£i captcha thÃ nh cÃ´ng")
        return code
    except Exception as e:
        print(f"âŒ Lá»—i khi giáº£i captcha: {e}")
        return None


def handle_captcha(browser):
    try:
        # Äá»£i iframe captcha hiá»‡n ra
        WebDriverWait(browser, 5).until(
            EC.frame_to_be_available_and_switch_to_it((By.CSS_SELECTOR, "iframe[src*='recaptcha']"))
        )
        iframe_src = browser.execute_script("return document.querySelector('iframe[src*=\"recaptcha\"]').src")
        parsed = urlparse(iframe_src)
        site_key = parse_qs(parsed.query)['k'][0]

        browser.switch_to.default_content()
        current_url = browser.current_url

        token = solve_recaptcha(site_key, current_url)
        if token:
            # Inject token vÃ o textarea áº©n cá»§a reCAPTCHA
            browser.execute_script(f'document.getElementById("g-recaptcha-response").innerHTML="{token}";')
            browser.execute_script("""
                var el = document.getElementById("g-recaptcha-response");
                el.style.display = "";
                el.dispatchEvent(new Event('change'));
            """)
            time.sleep(2)
            return True
        else:
            print("âŒ KhÃ´ng thá»ƒ giáº£i captcha.")
            return False
    except Exception:
        browser.switch_to.default_content()
        return True  # Náº¿u khÃ´ng tháº¥y captcha, tráº£ vá» True


def extract_job_details(browser, url):
    browser.get(url)

    # Náº¿u trang cÃ³ captcha, xá»­ lÃ½
    if not handle_captcha(browser):
        print(f"âš ï¸ KhÃ´ng thá»ƒ vÆ°á»£t captcha táº¡i {url}")
        return None, False

    try:
        WebDriverWait(browser, 30).until(
            EC.presence_of_element_located((By.ID, "job-title"))
        )
    except:
        print(f"âš ï¸ KhÃ´ng táº£i Ä‘Æ°á»£c trang chi tiáº¿t {url}")
        return None, False  # Tráº£ vá» False náº¿u lá»—i

    def safe_xpath(xpath):
        try:
            return browser.find_element(By.XPATH, xpath).text.strip()
        except:
            return "KhÃ´ng rÃµ"

    def safe_xpaths(xpath):
        try:
            elements = browser.find_elements(By.XPATH, xpath)
            return [el.text.strip() for el in elements if el.text.strip()]
        except:
            return []

    job_data = {
        "URL": url,
        "Tá»±a Ä‘á»": safe_xpath('//h1[@id="job-title"]'),
        "TÃªn cÃ´ng ty": safe_xpath('//p[contains(@class,"org-name")]/a/span'),
        "Má»©c lÆ°Æ¡ng": safe_xpath('//i[contains(@class, "cli-currency-circle-dollar")]/following-sibling::span'),
        "Äá»‹a Ä‘iá»ƒm": ", ".join(safe_xpaths(
            '//i[contains(@class, "cli-map-pin-line")]/following-sibling::span//a | //i[contains(@class, "cli-map-pin-line")]/following-sibling::span')),
        "Kinh nghiá»‡m": safe_xpath('//i[contains(@class, "cli-suitcase-simple")]/following-sibling::span'),
        "Tuá»•i": "",
        "Giá»›i tÃ­nh": "",
        "Cáº¥p báº­c": "",
        "Há»c váº¥n": "",
        "NgÃ nh nghá»": ""
    }

    summary_items = browser.find_elements(By.XPATH, '//div[contains(@class, "job-summary-item")]')
    for item in summary_items:
        try:
            label = item.find_element(By.CLASS_NAME, 'summary-label').text.strip()
            if label == "NgÃ nh nghá»":
                spans = item.find_elements(By.XPATH, './/div[contains(@class,"font-weight-bolder")]//span')
                job_data["NgÃ nh nghá»"] = ", ".join([s.text.strip() for s in spans if s.text.strip()])
            else:
                value = item.find_element(By.CLASS_NAME, 'font-weight-bolder').text.strip()
                if "Tuá»•i" in label:
                    job_data["Tuá»•i"] = value
                elif "Giá»›i tÃ­nh" in label:
                    job_data["Giá»›i tÃ­nh"] = value
                elif "Cáº¥p báº­c" in label:
                    job_data["Cáº¥p báº­c"] = value
                elif "Há»c váº¥n" in label:
                    job_data["Há»c váº¥n"] = value
        except:
            continue

    return job_data, True  # ThÃ nh cÃ´ng


def add_page_param(url, page_num):
    parsed_url = urlparse(url)
    query = parse_qs(parsed_url.query)
    query['page'] = [str(page_num)]
    new_query = urlencode(query, doseq=True)
    return urlunparse(parsed_url._replace(query=new_query))


if __name__ == "__main__":
    proxy = "192.168.2.19:20000"  # Proxy SOCKS5 cá»§a báº¡n
    browser = create_browser(proxy=proxy, proxy_type='socks5')

    list_page_url = "https://www.careerlink.vn/tim-viec-lam-tai/ho-chi-minh/HCM"

    try:
        max_pages = int(input("ğŸ”¢ Nháº­p sá»‘ trang muá»‘n thu tháº­p (nháº­p 0 Ä‘á»ƒ thu tháº­p toÃ n bá»™): "))
    except:
        max_pages = 0

    max_pages = max_pages if max_pages > 0 else None

    all_jobs = []  # Tá»•ng dá»¯ liá»‡u thu tháº­p

    save_folder = r"C:\Users\Admin\Documents\detaitotnghiep"
    os.makedirs(save_folder, exist_ok=True)
    save_path = os.path.join(save_folder, "all_job_details.csv")

    base_url = "https://www.careerlink.vn"
    page_num = 1

    while True:
        if max_pages and page_num > max_pages:
            print("ğŸ“Œ ÄÃ£ Ä‘áº¡t Ä‘áº¿n sá»‘ trang tá»‘i Ä‘a.")
            break

        page_url = add_page_param(list_page_url, page_num)
        browser.get(page_url)
        time.sleep(5)
        browser.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(4)

        elements = browser.find_elements(By.XPATH, '//a[contains(@href, "/tim-viec-lam/") and contains(@href, "/")]')
        page_links = []
        for el in elements:
            href = el.get_attribute("href")
            if href:
                full_url = urljoin(base_url, href)
                if full_url.startswith(base_url + "/tim-viec-lam/") and full_url not in page_links:
                    page_links.append(full_url)

        if not page_links:
            print("â›” KhÃ´ng tÃ¬m tháº¥y viá»‡c lÃ m má»›i trÃªn trang nÃ y, dá»«ng láº¡i.")
            break

        print(f"ğŸ” Trang {page_num} - tÃ¬m tháº¥y {len(page_links)} viá»‡c lÃ m")

        # QuÃ©t chi tiáº¿t tá»«ng job cá»§a page hiá»‡n táº¡i
        for idx, url in enumerate(page_links):
            print(f"â¡ï¸ Trang {page_num}, job {idx + 1}/{len(page_links)}: {url}")
            success = False
            retries = 0
            max_retries = 1  # Sá»‘ láº§n thá»­ láº¡i má»Ÿ trÃ¬nh duyá»‡t má»›i khi lá»—i

            while not success and retries <= max_retries:
                try:
                    job_data, success = extract_job_details(browser, url)
                    if success and job_data is not None:
                        all_jobs.append(job_data)
                    elif not success:
                        print("â™»ï¸ Restart trÃ¬nh duyá»‡t do lá»—i táº£i trang chi tiáº¿t...")
                        browser.quit()
                        time.sleep(3)
                        browser = create_browser(proxy=proxy, proxy_type='socks5')
                        retries += 1
                except Exception as e:
                    print(f"âš ï¸ Lá»—i táº¡i {url}: {e}")
                    break

            time.sleep(random.uniform(2, 4))

        # LÆ°u dá»¯ liá»‡u sau khi quÃ©t xong page
        try:
            if len(all_jobs) == 0:
                print("âŒ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ lÆ°u.")
            else:
                df = pd.DataFrame(all_jobs)
                df.to_csv(save_path, index=False, encoding="utf-8-sig")
                print(f"âœ… ÄÃ£ lÆ°u dá»¯ liá»‡u sau trang {page_num} vÃ o {save_path}")
        except Exception as e:
            print(f"Lá»—i khi lÆ°u file CSV: {e}")

        page_num += 1

    browser.quit()
